## Threads and Too much M I L K ! 2-5-20
- Threads

    - Share and communicate through glbal and static data, share the heap, each has its own stack and full use of the registers
    - Nire threads can be creates using either system calls (kernel threads) or libraries (user threads)
    - Differences from processes
        - Processes
            - A process is the abstraction used by the OS to manage resources and provide protection
            - A process defines an address space
                - Identifies all addresses that may be touched by the program
            - A process has a single thread of control that executes instructions sequentially
            - What if we decouple the thread of control information from the process??? 
                - Error: Thread will lose its access to code to run, environment variables, etc
            - Thread of instruction refers to just code and the way it flows around the functions, procedures, loops, and conditionals. Thread of controll refers to execution by kernel thread withits own program counter/stack. One thread of instruction can have many threads of control executing its code. 
            - Each process provides the resources needed to execute a program. A process has a virtual address space, executable code, open handles to system objects, a securit context, a unique process identifier, environment variables, a priority class, minimum and maximum working set sizes, and at least one thread of execution. Each process is started with a single threadm often called the primary thread, but can create additional threads from any of its threads. 
        - Threads 
            - Represents an abstract entity that executes a sequence of instructions
                - Short for thread of control
                - Deefines a single sequential execution stream within a process
            - Threads share their address space
            - Each process may have multiple threads of control
                - Must have one
                - Virtualizes the processor = runs the program with access to the resources provided by the process
            - A thread is an entity within a process that can be scheduled for execution. All threads of a process share its virtual address space and system resources. In addition, each thread maintains exception handlers, a scheduling priority, thread local storage, a unique thread identifier, and a set of structures the system will use to save the thread context until it is scheduled. The thread context included the thread's set of machine registers, the kerel stack, a thread environment vlock, and a user stack in the address space of the thread's process. Threads can also have their own security context (SYNCHRONIZATION), which can be used for 
            impersonating clients. 
            - Programmers create multi-threaded programs to:
                - Better represent the structure of the tasks
                - Improve performance
                    - One thread can perform computation while another waits for I/O
                    - Threads may be scheduled across different processors in a multi-processor architecture
                - Web servers
            - Threads are lightweight
                - Creating a thread is cheaper than creating a process
                - Communication between threads is easier than between processes
                    - Processes must set up a shared resource or pass messages or signals
                - Context switching between threads is cheaper because they share the same address space
            - Threads just like processes go through a sequence of new, ready, running, blocking, and terminated states
            - Threads and the address space
                - Processes define an address space; threads share teh address space
                    - All process data can be accessed by any thread
                        - Particularly global data
                        - Heap is also shared (What about pointers to heap? yes, because pointers can be stored in the thread's stack)
                - Each thread has:
                    - its own stack (stack pointers too)
                        - But there is no protection... so any thread can modify another thread's stack
                    - Exclusive use of the CPU registers while it is executing
                        - When a thread is pre-empted, its register values are saved as part of its state
                        - The new thread gets to use the registers! 
            - Metadata structures
                - Process Control Block contains process-specific information
                    - Owner, PID, heap pointer, priority, active thread and pointers to thread information
                - Thread control Block contains thread specific information
                    - Stack pointer, PC, thread state, register values, a pointer to PCB
    - User vs. kernel Threads
        - User-level Threads
            - A thread the OS does not know about
            - OS only schedules the process not the threads within a process
            - Programmer uses a thread library to manage threads (create, delete, synchronize, and schedule)
                - User-level code can define scheduling policy
                - Threads yield to other threads or voluntarily give up the processor
            - Switching USER threads does not involve a context switch
        - Kernel-Level Threads
            - A kernel-level thread is a thread that the OS knows about
                - Every process has at least one kernel-level thread
            - Kernel manages and schedules threads (as well as processes)
            - Switching between kernel-level threads of the same process requires a small context switch
                - Values of registers, program counter, and stack counter must be switched
                - Memory management nformation remains since threads share an address space
            - Also known as kernel threads 
            - THE POWER of kernel level threads
                - I/O: the OS can choose another thread in the smae process when a thread does I/O
                    - Non blocking calls are good in theroy, but difficult to program in practice
                - Kernel level threads can exploit parallelism
                    - Different processors of a symmetric multiprocessor
                    - Different cores on a multicore CPU
                - Used by systems: Linux, Solaris, Windows
        - Kernel-level Threads: Context switches between threads of the same process
            - Similar to proceses: 
                - Thread is running
                - Threads blocks, is interrupted, or voluntarily yields
                - Mode switch to kernel mode
                - OS code saves thread state to TCB
                - OS code chooses new thread to run
                - OS code loads its state from TCB
                - Mode switch to user mode
                - Thread is running
    - Creating, dispatching
    - One Abstraction, Many Flavors
        - Single-threaded processes
        - Multi-threaded processes with user-level threads
        - Multi-threaded proceses with kernel-level threads
        - In-kernel threads
    - Independent vs.Cooperating
        - Independent threads have no shared state with other threads
            - Simple to implement
            - Deterministic
            - Reproducible
            - Scheduling order doesn't matter
        - Cooperating threads share state
            - Non deterministic 
            - Non reproducible
            - Give us concurrency!
    - Race COnditions
        - What guarantees do we have about how our people/threads will scheduled? There is no way
        - A race condition occurs when two or more threads can access shared data and they try to change it at the same time. Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the threads will attempt to access the shared data. Therefore, the result of the change in data is dependent on the thread scheduling algorithm, i.e. both threads are "racing" to access/change the data.
## Locks and Semaphores Feb 10, 2020
- Critical sections: parts of the program where the shared resource is accessed need to be protected in ways that avoid the concurrent access.
- More about Race Conditions
- Eliminating race conditions! Or Forcing threads to behave properly
- Synchronization Terminology
    - Atomic Operations
    - Mutual exclusion, critical sections (Safety, Liveness, Bounded Waiting)
    - Synchronization Primitives
        - Locks
        - Semaphores
- Critical Sections and Correctness
    - Four properties are required for correctness
        - 1. Safety: only one thread in the critical section
        - 2. Liveness: if no threads are executing a critical section, and a thread wishes to enter a critical section, that thread must be guaranteed to eventually enter the critical section
        - 3. Bounded Waiting: if a thread wishes to enter a critical section, then there exists a bound on the number of other threads that may enter the critical section before that thread does
        - 4. Failure Atomicity: it's okay for a thread to die in the critical section
- Mutual Exclusion
    - Exactly one thread (or process) is doing a particular activity at a time. Usually related to critical sections
    - Some computer resources cannot be accessed by multiple threads at a time
    - For shared memory architectures, data structures are often mutuall exclusive
        - Two threads adding to a linked list can corrupt the list
- When to use Mutual Exclusion/Critical Sections?
    - Anytime you access shared data
        - If a thread checks a value 
        - If a thread updates a piece of shared data
- Terminology 
    - Mutual Exclusion: Exactly one thread (or process) is doing a particular activity at a time. Usually related to critical sections
    - Critical Section: A piece of code that only one thread can execute at a time
    - Atomic Operation: An operation that is uninterruptible
    - Synchronization: Using atomic operations to ensure cooperation between threads
- Atomic Operations
    - Operations that are uninterruptible --- run to completion or not at all

- 6 commandments
    - Thou shalt always do things the same way
    - Thou shalt always synchronize with locks and condition variables
    - Thou shalt always acquire the lock at the beginning of a function and release it at the
    end
    - Thou shalt always hold lock when operating on a condition variable
    - Thou shalt always wait in a while loop
    - (Almost) Never sleep()

## Locks
- Locks, Generally allows one thread to prevent another thread from doing something
    - Lock before entering a critical section or before accessing shared data
    - Unlock when leaving a critical section or when access to shared data is complete
    - Wait if locked
- Locks, more formally provide mutual exclusion to shared data with two atomic routines:
        - Lock::acquire -> wait until lock is free, then grab it
        - Lock::release - unlock and wak up any thread waiting in acquire
    - Locks have two states: Busy and Free
        - Lock is initially free
    - Rules for using a lock:
        - Acquire the lock before accessing shared data
        - Release the lock after finishing with shared data
- Key observation 
    - Why do we need mutual exclusion?
        - scheduler (prevent context switching at bad times)
    - On a uniprocessor, an operation is atomic if no context switch can occur in the middle fo the operation
        - Mutual exclusion by preventing the context switch
    - Context switches occur because of:
        - Internal events: systems calls and exceptions
        - External events: interrupts
- Disabling interrupts
    - Tells the hardware to delay handling any external events until after the thread is finished modifying the critical section
    - In some implementations, done by setting and unseting the interrupt status bit
    - Why do we have interrupts
- Atomic Read-Modify-Write Instructions
    - Atomic read-modify-write instructions atomically read a value from memory into a register and write a new value
        - Read a memory location into a register AND
        - Write a new value to location
        - Test&set
- Locks provide mutual exclusion
    - Protext critical sections
    - Implementing them may require a critical section
    - But we need more
        - What if we need to wait for another thread to take action?
        - What if there is more than one resource available?

## Semaphores
- Semaphores offer elegant solutions to synchronization
    - Mutual exclusion and more general synchronization
- Semaphores are basically generalized locks
    - Support two atomic operations up & down
    - Has a value, bit the value has more options than busy/free
    - Supports a queue of threads that are waiting to access a resource
- Two Types of semaphores
    - Binary: 
        - Same as a lock
        - Guarantees mutually exclusive access to a resource
        - Has two values: 0 or 1
        - Initial value is always free (1)
    - Counted Semaphore
        - Represents a resource with many units available 
        - Initial count is typically the number of resources
            - Always a non-negative integer
        - Allows a thread to continue as long as more instances are available
        - Used for more general synchronization
- When to use semaphores
    - Mutual Exclusion
        - Use to protect the critical section 
    - Control access to a pool of resouces -> counted semaphore
    - General Synchronization
        - Use to enforce general scheduling constraints where the threads must wait for some circumstance
        - Value is typically 0 start
        - Down() and up() are called different threads from different parts of the code base

- Locks define critical sections
    - Lock implementation generally requires hardware support
    - Locks can busy-wait, and busy-waiting cheaply is important
- Semaphores are basically generalized locks
    - Used for mutual exclusion and more general synchronization
    - Each sempahore supports a queue of processes that are waiting to access a critical section
    - No busy waiting! Threads sleep inside down() until they have the resource 

- When to use semaphores
    - Mutual exclusion
    - control access to a pool of resources (more than one resource)
    - General synchronization
- CONS of semaphores
    - Shared global bariables
    - Too many purposes
    - No control or guarantee of proper usage
    - Difficult to read code

- Deadlock occurs when two or more threads are waiting for an event that can only be generated by these same threads
    - starvation can occur without deadlck
    - Deadlock does imply starvation
    - CONDITIONS
        - Mutual exclusion: at least one thread must hold a resource in non-sharable mode
            - Make resources sharable
        - Hold and wait: At least one thread holds a resources and is waiting for other 
        resources to become available. A different thread holds the resource. 
            - Guarantee a thread cannot hold one resource when it request another
        - No preemption: A thread only releases a resource voluntarily; another thread or the OS cannot force the thread to release the resource
            - If a thread requests a resource that cannot be immediately allocated to it, then the OS preempts all the resources the thread is currently holding. Only when all the resources are available will the OS restart the thread
        - circular wait: A set of waiting threads where t is waiting on t and t is waiting on t. 
            - impose an ordering on the locks and request them in order
                - Complications:
                    - Maintaining global order is difficult in a large project
                    - Global order can force a client to grab a lock earlier than it would like, tying up the lock for longer than necessary

## Monitors
- Encapsulate shared data
    - Collect related shared data into an object/module
    - all data is private
- Allow operations on the shared data
- Provide mutual exclusion
- Allow threads to synchronize in the critical section

- A monitor defines a lock and zero or more condition variables for managing concurrent access to shared data.
    - Uses the lock to ensure that only a single thread is active in the monitor at any point
    - the lock also provides mutual exclusion for shared data
    - Condition variables enable threads to block waiting for an event inside of critical sections
        - Release the lock at the same time the thread is put to sleep

- Condition variables 
    - Enable threads to wait efficiently for changes to shared state protected by a lock
    - Each one is a queue of waiting threads
    - Enable the thread to block inside a critical section by atomically releasing the Lock at the same time the thread is blocked
    - Rule: A thread must hold the lock when doing condition variable operations

- Mesa/Hansen Style
    - The thread that signals keeps the lock
    - The waiting thread waits for the lock
        - Signal is only a hint that the condition mau be true: shared state may have changed!
        - Adding signals affects performance, but never safety
    - Implemented in Java and most real operating systems

- Signal vs Broadcast
    - It is always safe to use broadcast() instead of signal()
        - only performacne is affected
    - Signal is preferable when
        - at most one waiting thread can make progress
        - any thread waiting on the condition variable can make progress
    - broadcast is preferable when
        - multiple waiiting threads may be able to make progress
        - the same condition variable is used for multiple predicates
    
## Advanced Synchronization and Synchronization Recap
- One Big Lock
    - Advantage: Simple
        -  Relatively easy to get correct
        - This is a big advantage
    - Disadvantage: Performance
        - Eliminates advantages due to multi-threading for that part of your code
        - Eliminates advantages due to multicore in that part of your code

- Fine grained locking
    - Better for performance
        - This will matter more in the kernel than in an application
    - Complex
        - May need to acquire multiple locks to accomplish a task
        - Incorrect code becomes more likely -> deadlock

- Conservative Two-phase locking
    - A thread must:
        - acquire all lockts it will need
            - If all locks cannot be acquired, release any already acquired and begin again
    - Thus B cannot see any of A's changes until A commits and releases he lock
    - provides serailizability
    - Prevents deadlock
- Transactions
    - Transactions group actions together so that they are
        - Atomic: they all happen or they all don't
        - Serializable: transactions appear to happen one after the other
        - Durable: once it happens, it sticks
    - Critical sections give us atomicity and serializability, but not durability!!!!!

- Achieving Durability, In Reality
    - Reversing changes on disk is hard
    - Keep write ahead log on disk of all changes in teh transaction
    - Once all changes are written to the log, the transaction is committed
        - Which means the word "Commit" is written at the end
        - If a crash happens before the commit, then the log can just be ignored
    - Write-behind changes to the disk
        - As in, later on a thread will actaully put those changes in their proper spot in the filesystem
        - If a crash occurs during 
